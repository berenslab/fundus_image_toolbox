{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from models.model_utils import process_image\n",
    "from utils.notebook_utils import apply_temperature, pixel_uncertainty\n",
    "from utils.notebook_utils import softmax, estimate_dice_li, get_patches\n",
    "from utils.notebook_utils import scale_temp_dice, apply_temperature\n",
    "from models.model_utils import dice_metric\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import yaml\n",
    "from munch import munchify\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure manually what to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_suffix = 'all_patches'\n",
    "run_again = False # whether to re-compute results or load from pre-computed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Experiments Configs\n",
    "with open('config_' + config_suffix + '.yaml', 'r') as file:\n",
    "        cfg = munchify(yaml.load(file, Loader=yaml.FullLoader))\n",
    "        file.close()\n",
    "\n",
    "with open('paths.yaml', 'r') as file0:\n",
    "        cfg_paths = munchify(yaml.load(file0, Loader=yaml.FullLoader))\n",
    "        file0.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test\n",
    "if cfg.patch_size % 2  == 0:\n",
    "    warnings.warn('Please specify an uneven patchsize in config.yaml.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Forward Passes and Compute Uncertainty Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ensemble Passes\n",
    "with open('cache/ensemble_dict_val.pkl', 'rb') as file2:\n",
    "    ensemble_dict = pkl.load(file2)\n",
    "    # ensemble_dict['logits'] contains list of m (no. ensemble members), where\n",
    "    # each list element contains list of n_val (no. of validation images)\n",
    "    file2.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Predictions and Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_dict['mean logits'] = np.array(ensemble_dict['logits']).mean(axis=0)\n",
    "ensemble_dict['mean predictions'] = softmax(np.array(ensemble_dict['logits'])).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_arr = softmax(np.array(ensemble_dict['logits'])) # all predictions as array\n",
    "mean_true_dice = np.array(ensemble_dict['true dice']).mean(axis=0)\n",
    "pos_pred = (ensemble_dict['mean predictions'] > 0.5).mean(axis=(1,2))\n",
    "\n",
    "# convert probabilities to logits for whole images\n",
    "predicted_logits_arr = np.log(predictions_arr / (1 - predictions_arr + 10e-9)) # could simply use ensemble_dict['logits'], no?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_maps, variance_maps = [], []\n",
    "\n",
    "for i in range(predictions_arr.shape[1]):\n",
    "    entropy_maps.append(pixel_uncertainty(predictions_arr[:, i,: ,:], 'entropy'))\n",
    "    variance_maps.append(pixel_uncertainty(predictions_arr[:, i,: ,:], 'variance'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct validation split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct train and val split\n",
    "root = cfg_paths.FIVES\n",
    "train_x = sorted(glob(os.path.join(root, 'train/Original/*')))\n",
    "train_y = sorted(glob(os.path.join(root, 'train/Ground truth/*')))\n",
    "\n",
    "validation_split = .2\n",
    "indices = list(range(len(train_x)))\n",
    "split = int(np.floor(validation_split * len(train_x)))\n",
    "np.random.seed(23) # manually confirmed with the training script\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "val_x = train_x[:split]\n",
    "val_y = train_y[:split]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions which estimate dice for the remaining images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_uncertainty(uncertainty_map, patch_size, stride=1):\n",
    "    \"\"\"\n",
    "        ::param::    \n",
    "        uncertainty_map: [np.array] of shape nxn\n",
    "        patch_size: [int] defines side length of square patch\n",
    "        stride: [int]\n",
    "        \n",
    "        ::return::\n",
    "        Patch Identifier with Convolution Value\n",
    "    \"\"\"\n",
    "\n",
    "    patch_convolved = []\n",
    "    reference_points = [] # identifies patches\n",
    "    patch_dict = {'reference': reference_points,\n",
    "                  'patch uncertainty': patch_convolved}\n",
    "    \n",
    "\n",
    "    def sliding_window(arr, step_size=stride, window_size=patch_size):\n",
    "        \"\"\"\n",
    "            Iterator which yields a binary mask for patch extraction\n",
    "            alongside identifying coordinates of the reference point.\n",
    "        \"\"\"\n",
    "        for y in range(0, arr.shape[0] - patch_size, step_size):\n",
    "            for x in range(0, arr.shape[1] - patch_size, step_size):\n",
    "\n",
    "                bool_img = np.zeros_like(arr) * False\n",
    "                bool_img[y:y + window_size, x:x + window_size] = True\n",
    "                \n",
    "                yield (y, x, bool_img.astype(bool))\n",
    "\n",
    "    windows = sliding_window(uncertainty_map, stride, patch_size)\n",
    "    \n",
    "\n",
    "    for y, x, window in windows:\n",
    "        patch_value = uncertainty_map[window].sum()\n",
    "        patch_convolved.append(patch_value)\n",
    "        reference_points.append((y, x))\n",
    "\n",
    "    return patch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimation_info(img_idx, cfg):\n",
    "    # Image and GT\n",
    "    x_path = val_x[img_idx]\n",
    "    y_path = val_y[img_idx]\n",
    "\n",
    "    image = cv2.imread(x_path, cv2.IMREAD_COLOR) ## (512, 512, 3)\n",
    "    # image = clahe_equalized(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (512,512))\n",
    "\n",
    "    mask = cv2.imread(y_path, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
    "    mask = cv2.resize(mask, (512,512)) # interpolation =  INTER_NEAREST\n",
    "\n",
    "    x_image, y_image = process_image(image, mask)\n",
    "    gt = y_image.squeeze()\n",
    "\n",
    "    if cfg.uncertainty_type == 'entropy':\n",
    "        uncertainty_map = - entropy_maps[img_idx]\n",
    "    elif cfg.uncertainty_type == 'variance':\n",
    "        uncertainty_map = variance_maps[img_idx]\n",
    "\n",
    "    convolution_dict = convolve_uncertainty(uncertainty_map, cfg.patch_size - 1)\n",
    "    convolve_map = np.array(convolution_dict['patch uncertainty']).reshape(432, 432)\n",
    "\n",
    "    row, col = np.unravel_index(np.argmax(convolve_map, axis=None), convolve_map.shape)\n",
    "    in_image = np.ones_like(convolve_map).astype(bool)\n",
    "\n",
    "    ref_array = np.array(convolution_dict['reference'])\n",
    "\n",
    "    no_patches = cfg.no_patches\n",
    "    # available to choose from\n",
    "    in_image = np.ones_like(convolve_map.flatten()).astype(bool)\n",
    "    patch_references = []\n",
    "\n",
    "    for i in range(cfg.no_patches - 1):\n",
    "        # choose pixel with largest value available\n",
    "        max_idx_flat = convolve_map.flatten()[in_image].argmax()\n",
    "        # store reference point (in fullsize image space) for the patch\n",
    "        reference_point = np.array(convolution_dict['reference'])[in_image.flatten(), :][max_idx_flat, :]\n",
    "        patch_references.append(reference_point)\n",
    "\n",
    "        # update in_image, i.e. \"remove\" all pixels that are already selected by earlier patches\n",
    "        y_constraint = np.logical_and(ref_array[:, 0] <= reference_point[0] + cfg.patch_size, ref_array[:, 0] >= reference_point[0] - cfg.patch_size)\n",
    "        x_constraint = np.logical_and(ref_array[:, 1] <= reference_point[1] + cfg.patch_size, ref_array[:, 1] >= reference_point[1] - cfg.patch_size)\n",
    "        patch_constraint = np.logical_and(y_constraint, x_constraint)\n",
    "\n",
    "        in_image[patch_constraint] = False\n",
    "\n",
    "\n",
    "    prediction = predictions_arr[0, img_idx,:,:] > 0.5\n",
    "    masked_out = np.zeros_like(prediction).astype(bool)\n",
    "    dsc_curve = []\n",
    "    est_dsc_curve = []\n",
    "    est_w = []\n",
    "    true_w = []\n",
    "    est_dice_remain_list = []\n",
    "    dice_remain_list = []\n",
    "    pos_remain = []\n",
    "    masks = []\n",
    "\n",
    "    for i in range(len(patch_references) + 1):\n",
    "\n",
    "        masks.append(~masked_out)\n",
    "\n",
    "        dice_remain = dice_metric(gt[~masked_out], prediction[~masked_out])\n",
    "        est_dice_remain = estimate_dice_li(predictions_arr[0, img_idx,:,:][~masked_out])\n",
    "\n",
    "        est_dice_remain_list.append(est_dice_remain)\n",
    "        dice_remain_list.append(dice_remain)\n",
    "        pos_remain.append((predictions_arr[0, img_idx, :,:][~masked_out] > 0.5).mean())\n",
    "\n",
    "        # update mask\n",
    "        if i <= len(patch_references) - 1:\n",
    "            patch_ref = patch_references[i]\n",
    "            masked_out[patch_ref[0]:patch_ref[0] + cfg.patch_size, patch_ref[1]:patch_ref[1] + cfg.patch_size] = True\n",
    "\n",
    "\n",
    "    return {'true dice': dice_remain_list, 'est dice': est_dice_remain_list,\n",
    "            'positive remaining': pos_remain, 'masks': masks}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potentially select subset of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected 120 images.\n"
     ]
    }
   ],
   "source": [
    "# Select Relevant Images\n",
    "# Select relevant images\n",
    "foreground_thresh = cfg.fg_thresh\n",
    "dice_thresh = cfg.dice_thresh\n",
    "\n",
    "sufficient_foreground = pos_pred > foreground_thresh\n",
    "sufficient_dice = mean_true_dice > dice_thresh\n",
    "\n",
    "image_selection = np.logical_and(sufficient_foreground, sufficient_dice)\n",
    "image_selection_ids = np.where(image_selection)[0]\n",
    "print('You have selected ' + str(image_selection.sum()) + ' images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate DSC and measure true DSC for remaining images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_again:\n",
    "\n",
    "    estimation_info_val = []\n",
    "\n",
    "    for img_id in image_selection_ids:\n",
    "        estimation_info_val.append(get_estimation_info(img_id, cfg=cfg)) # esentially returns the orange and blue curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save estimated DSC of remaining images or load from previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_again:\n",
    "    with open('cache/' + config_suffix + '_estimation_info_val.pkl', 'wb') as file:\n",
    "        pkl.dump(estimation_info_val, file)\n",
    "    file.close()\n",
    "else:\n",
    "    with open('cache/' + config_suffix + '_estimation_info_val.pkl', 'rb') as file:\n",
    "        estimation_info_val = pkl.load(file)\n",
    "    file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate: Fit Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_logits = predicted_logits_arr.mean(axis=0)[image_selection_ids, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose logits of cut out images\n",
    "logit_remain = [] \n",
    "\n",
    "for i, one_img_logit in enumerate(relevant_logits):\n",
    "\n",
    "    for mask in estimation_info_val[i]['masks']:\n",
    "\n",
    "        logit_remain.append(one_img_logit[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dice_flat = [el for sublist in estimation_info_val for el in sublist['true dice']]\n",
    "if run_again:\n",
    "    T_remain = scale_temp_dice(logit_remain, np.array(true_dice_flat).mean()) \n",
    "else:\n",
    "    T_remain = 1.2993164062499996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cache/' + config_suffix + '_optimal_T.pkl', 'wb') as file:\n",
    "    pkl.dump(T_remain, file)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chest_x-rays",
   "language": "python",
   "name": "chest_x-rays"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
